{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.5.1\n",
      "Commit 697e782ab8 (2020-08-25 20:08 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin19.5.0)\n",
      "  CPU: Intel(R) Core(TM) i5-5257U CPU @ 2.70GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-9.0.1 (ORCJIT, broadwell)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Policies with Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Assignment 2. This notebook will help you understand:\n",
    "- Policy Evaluation and Policy Improvement.\n",
    "- Value and Policy Iteration.\n",
    "- Bellman Equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld City\n",
    "\n",
    "Gridworld City, a thriving metropolis with a booming technology industry, has recently experienced an influx of grid-loving software engineers. Unfortunately, the city's street parking system, which charges a fixed rate, is struggling to keep up with the increased demand. To address this, the city council has decided to modify the pricing scheme to better promote social welfare. In general, the city considers social welfare higher when more parking is being used, the exception being that the city prefers that at least one spot is left unoccupied (so that it is available in case someone really needs it). The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. Now the city has hired you &mdash; an expert in dynamic programming &mdash; to help determine an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the city council's parking MDP, states are nonnegative integers indicating how many parking spaces are occupied, actions are nonnegative integers designating the price of street parking, the reward is a real value describing the city's preference for the situation, and time is discretized by hour. As might be expected, charging a high price is likely to decrease occupancy over the hour, while charging a low price is likely to increase it.\n",
    "\n",
    "For now, let's consider an environment with three parking spaces and three price points. Note that an environment with three parking spaces actually has four states &mdash; zero, one, two, or three spaces could be occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_ (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StatsBase\n",
    "using PyCall\n",
    "\n",
    "mutable struct ParkingWorld\n",
    "    num_spaces::Int64\n",
    "    num_prices::Int64\n",
    "    price_factor::Float64\n",
    "    occupants_factor::Float64\n",
    "    null_factor::Float64\n",
    "    State::Array{Int64,1}\n",
    "    Action::Array{Int64,1}\n",
    "    function ParkingWorld(;num_spaces = 10, num_prices = 4, price_factor = 0.1,\n",
    "                          occupants_factor = 1.0, null_factor = 1/3)\n",
    "        parkingworld = new()\n",
    "        parkingworld.num_spaces = num_spaces\n",
    "        parkingworld.num_prices = num_prices\n",
    "        parkingworld.price_factor = price_factor\n",
    "        parkingworld.null_factor = 1/3\n",
    "        parkingworld.occupants_factor = occupants_factor\n",
    "        parkingworld.State = [num_occupied for num_occupied in 0:num_spaces]\n",
    "        parkingworld.Action = collect(0:num_prices-1)\n",
    "        parkingworld\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function policy(park::ParkingWorld, state_, reward, state, action)\n",
    "    if reward != get_reward(park, state, state_)\n",
    "        return 0\n",
    "    else\n",
    "        center = (1 - park.price_factor) * state +\n",
    "                    park.price_factor * park.num_spaces * (1 - action/park.num_prices)\n",
    "        emphasis = exp.(-abs.(range(0, stop = (2 * park.num_spaces-1)) .- center) ./ 5)\n",
    "        if state_ == park.num_spaces\n",
    "            return sum(emphasis[state_+1:end]) / sum(emphasis)\n",
    "        end\n",
    "        return emphasis[state_+1] / sum(emphasis)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function support(park::ParkingWorld, state, action)\n",
    "    return [(state_, get_reward(park, state, state_)) for state_ in park.State]\n",
    "end\n",
    "\n",
    "function transitions(park::ParkingWorld, state, action)\n",
    "    trans = [[reward, policy(park, state_, reward, state, action)]\n",
    "                for (state_, reward) in support(park, state, action)]\n",
    "    tmp = trans[1]\n",
    "    for i in 2:length(trans)\n",
    "        tmp = hcat(tmp, trans[i])\n",
    "    end\n",
    "    trans = tmp'\n",
    "    return trans\n",
    "end\n",
    "\n",
    "\n",
    "function get_reward(park::ParkingWorld, state, state_)\n",
    "    return state_reward(park, state) + state_reward(park, state_)\n",
    "end\n",
    "\n",
    "function state_reward(park::ParkingWorld, state)\n",
    "    if state == park.num_spaces\n",
    "        return park.null_factor * state * park.occupants_factor\n",
    "    else\n",
    "        return state * park.occupants_factor\n",
    "    end\n",
    "end\n",
    "\n",
    "function random_state(park::ParkingWorld)\n",
    "    return rand(0:(park.num_prices-1))\n",
    "end\n",
    "\n",
    "\n",
    "function step_(park::ParkingWorld, state, action)\n",
    "    probabilities = [\n",
    "        policy(park, state_, reward(park, state, state_), state, action) for state_ in park.State\n",
    "    ]\n",
    "    return sample(park.State, Weights(probabilities))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.75\n",
       " 0.21\n",
       " 0.04"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_spaces = 3\n",
    "num_prices = 3\n",
    "env = ParkingWorld(num_spaces = num_spaces, num_prices = num_prices)\n",
    "\n",
    "# The value function is a one-dimensional array where the  i-th entry gives the value of  i\n",
    "# spaces being occupied.\n",
    "V = zeros(num_spaces + 1)\n",
    "pi_array = ones(num_spaces + 1, num_prices) ./ num_prices\n",
    "\n",
    "state = 1\n",
    "V[state] = 10\n",
    "pi_array[1,:] = [0.75, 0.21, 0.04]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Array{Float64,2}:\n",
       " 0.75      0.21      0.04\n",
       " 0.333333  0.333333  0.333333\n",
       " 0.333333  0.333333  0.333333\n",
       " 0.333333  0.333333  0.333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi(A=1|S=0) = 0.75    pi(A=2|S=0) = 0.21    pi(A=3|S=0) = 0.04    \n",
      "pi(A=1|S=1) = 0.33    pi(A=2|S=1) = 0.33    pi(A=3|S=1) = 0.33    \n",
      "pi(A=1|S=2) = 0.33    pi(A=2|S=2) = 0.33    pi(A=3|S=2) = 0.33    \n",
      "pi(A=1|S=3) = 0.33    pi(A=2|S=3) = 0.33    pi(A=3|S=3) = 0.33    \n"
     ]
    }
   ],
   "source": [
    "for i in 1:size(pi_array,1)\n",
    "    tmp = pi_array[i, :]\n",
    "    println(join(\"pi(A=$a|S=$(i-1)) = $(round(p, digits=2))\" * repeat(\" \",4) for (a, p) in enumerate(tmp)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <module 'mpl_toolkits.axes_grid1' from '/Users/swami/.julia/Conda_env/lib/python3.8/site-packages/mpl_toolkits/axes_grid1/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np = pyimport(\"numpy\")\n",
    "plt = pyimport(\"matplotlib.pyplot\")\n",
    "ticker = pyimport(\"matplotlib.ticker\")\n",
    "axes_grid1 = pyimport(\"mpl_toolkits.axes_grid1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"font\", size=30)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=25)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=25)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=17)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=17)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=20)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=30)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_tool (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function plot_tool(V, pi)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12.5, 5))\n",
    "    ax1.axis(\"on\")\n",
    "    ax1.cla()\n",
    "    states = np.arange(size(V, 1))\n",
    "    ax1.bar(states, V, edgecolor=\"none\")\n",
    "    ax1.set_xlabel(\"State\")\n",
    "    ax1.set_ylabel(\"Value\", rotation=\"horizontal\", ha=\"right\")\n",
    "    ax1.set_title(\"Value Function\")\n",
    "    ax1.xaxis.set_major_locator(ticker.MaxNLocator(integer=true, nbins=6))\n",
    "    ax1.yaxis.grid()\n",
    "    ax1.set_ylim(bottom=minimum(V))\n",
    "    # plot policy\n",
    "    ax2.axis(\"on\")\n",
    "    ax2.cla()\n",
    "    im = ax2.imshow(pi', cmap=\"Greys\", vmin=0, vmax=1, aspect=\"auto\")\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.set_xlabel(\"State\")\n",
    "    ax2.set_ylabel(\"Action\", rotation=\"horizontal\", ha=\"right\")\n",
    "    ax2.set_title(\"Policy\")\n",
    "    start, last = ax2.get_xlim()\n",
    "    ax2.xaxis.set_ticks(np.arange(start, last), minor=true)\n",
    "    ax2.xaxis.set_major_locator(ticker.MaxNLocator(integer=true, nbins=6))\n",
    "    ax2.yaxis.set_major_locator(ticker.MaxNLocator(integer=true, nbins=6))\n",
    "    start, last = ax2.get_ylim()\n",
    "    ax2.yaxis.set_ticks(np.arange(start, last), minor=true)\n",
    "    ax2.grid(which=\"minor\")\n",
    "    divider = axes_grid1.make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.20)\n",
    "    cbar = fig.colorbar(im, cax=cax, orientation=\"vertical\")\n",
    "    cbar.set_label(\"Probability\", rotation=0, ha=\"left\")\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    sleep(0.001)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tool(V, pi_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left, the value function is displayed as a barplot. State zero has an expected return of ten, while the other states have an expected return of zero. On the right, the policy is displayed on a two-dimensional grid. Each vertical strip gives the policy at the labeled state. In state zero, action zero is the darkest because the agent's policy makes this choice with the highest probability. In the other states the agent has the equiprobable policy, so the vertical strips are colored uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(S'=1, R=1.0 | S=3, A = 1)=0.12\n",
      "p(S'=2, R=2.0 | S=3, A = 1)=0.15\n",
      "p(S'=3, R=3.0 | S=3, A = 1)=0.18\n",
      "p(S'=4, R=2.0 | S=3, A = 1)=0.54\n"
     ]
    }
   ],
   "source": [
    "state = 3\n",
    "action = 1\n",
    "\n",
    "transition = transitions(env, state, action)\n",
    "\n",
    "\n",
    "for i in 1:size(transition,1)\n",
    "    tmp = transition[i, :]\n",
    "    println(\"p(S\\'=$i, R=$(tmp[1]) | S=$state, A = $action)=$(round(tmp[2], digits=2))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Policy Evaluation\n",
    "\n",
    "First, the city council would like you to evaluate the quality of the existing pricing scheme. Policy evaluation works by iteratively applying the Bellman equation for $v_{\\pi}$ to a working value function, as an update rule, as shown below.\n",
    "\n",
    "$$\\large v(s) \\leftarrow \\sum_a \\pi(a | s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v(s')]$$\n",
    "This update can either occur \"in-place\" (i.e. the update rule is sequentially applied to each state) or with \"two-arrays\" (i.e. the update rule is simultaneously applied to each state). Both versions converge to $v_{\\pi}$ but the in-place version usually converges faster. **In this assignment, we will be implementing all update rules in-place**, as is done in the pseudocode of chapter 4 of the textbook. \n",
    "\n",
    "We have written an outline of the policy evaluation algorithm described in chapter 4.1 of the textbook. It is left to you to fill in the `bellman_update` function to complete the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate_policy(env::ParkingWorld, V, pi_array, gamma, theta)\n",
    "    delta = Inf\n",
    "    while delta > theta\n",
    "        delta = 0\n",
    "        for s in env.State\n",
    "            v = V[s+1]\n",
    "            bellman_update(env, V, pi_array, s, gamma)\n",
    "            delta = max(delta, abs(v - V[s+1]))\n",
    "        end\n",
    "    end\n",
    "    return V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bellman_update (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bellman_update(env::ParkingWorld, V, pi_array, s, gamma)\n",
    "    # Mutate V according to the Bellman update equation\n",
    "    state_value = 0\n",
    "    for action in env.Action\n",
    "        trans = transitions(env, s, action) # trainsition value\n",
    "        state_value_tmp = 0\n",
    "        for state in env.State\n",
    "            state_value_tmp += trans[state+1, 2] * (trans[state+1,1] +\n",
    "                                                    gamma * V[state+1])\n",
    "        end\n",
    "        state_value += pi_array[s+1, action+1] * state_value_tmp\n",
    "    end\n",
    "    V[s+1] = state_value\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below uses the policy evaluation algorithm to evaluate the city's policy, which charges a constant price of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_spaces = 10\n",
    "num_prices = 4\n",
    "env = ParkingWorld(num_spaces = num_spaces, num_prices=num_prices)\n",
    "V = zeros(num_spaces+1)\n",
    "city_policy = zeros(num_spaces+1, num_prices)\n",
    "city_policy[:,2].=1\n",
    "gamma = 0.9\n",
    "theta = 0.1\n",
    "V = evaluate_policy(env, V, city_policy, gamma, theta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Int64,1}:\n",
       "  0\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  5\n",
       "  6\n",
       "  7\n",
       "  8\n",
       "  9\n",
       " 10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Float64,1}:\n",
       " 80.04173398685609\n",
       " 81.65532302729444\n",
       " 83.37394007142157\n",
       " 85.12975565977709\n",
       " 86.87174913186462\n",
       " 88.55589131273771\n",
       " 90.14020421970433\n",
       " 91.58180605347238\n",
       " 92.81929841429525\n",
       " 93.78915889368793\n",
       " 87.7779299121454"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11×4 Array{Float64,2}:\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0\n",
       " 0.0  1.0  0.0  0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tool(V, city_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the output (rounded to one decimal place) against the answer below:<br>\n",
    "State $\\quad\\quad$    Value<br>\n",
    "0 $\\quad\\quad\\quad\\;$        80.0<br>\n",
    "1 $\\quad\\quad\\quad\\;$        81.7<br>\n",
    "2 $\\quad\\quad\\quad\\;$        83.4<br>\n",
    "3 $\\quad\\quad\\quad\\;$        85.1<br>\n",
    "4 $\\quad\\quad\\quad\\;$        86.9<br>\n",
    "5 $\\quad\\quad\\quad\\;$        88.6<br>\n",
    "6 $\\quad\\quad\\quad\\;$        90.1<br>\n",
    "7 $\\quad\\quad\\quad\\;$        91.6<br>\n",
    "8 $\\quad\\quad\\quad\\;$        92.8<br>\n",
    "9 $\\quad\\quad\\quad\\;$        93.8<br>\n",
    "10 $\\quad\\quad\\;\\;\\,\\,$       87.8<br>\n",
    "\n",
    "Observe that the value function qualitatively resembles the city council's preferences &mdash; it monotonically increases as more parking is used, until there is no parking left, in which case the value is lower. Because of the relatively simple reward function (more reward is accrued when many but not all parking spots are taken and less reward is accrued when few or all parking spots are taken) and the highly stochastic dynamics function (each state has positive probability of being reached each time step) the value functions of most policies will qualitatively resemble this graph. However, depending on the intelligence of the policy, the scale of the graph will differ. In other words, better policies will increase the expected return at every state rather than changing the relative desirability of the states. Intuitively, the value of a less desirable state can be increased by making it less likely to remain in a less desirable state. Similarly, the value of a more desirable state can be increased by making it more likely to remain in a more desirable state. That is to say, good policies are policies that spend more time in desirable states and less time in undesirable states. As we will see in this assignment, such a steady state distribution is achieved by setting the price to be low in low occupancy states (so that the occupancy will increase) and setting the price high when occupancy is high (so that full occupancy will be avoided)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Policy Iteration\n",
    "Now the city council would like you to compute a more efficient policy using policy iteration. Policy iteration works by alternating between evaluating the existing policy and making the policy greedy with respect to the existing value function. We have written an outline of the policy iteration algorithm described in chapter 4.3 of the textbook. We will make use of the policy evaluation algorithm you completed in section 1. It is left to you to fill in the `q_greedify_policy` function, such that it modifies the policy at $s$ to be greedy with respect to the q-values at $s$, to complete the policy improvement algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "improve_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function improve_policy(env::ParkingWorld, V, pi_array, gamma)\n",
    "    policy_stable = true\n",
    "    for s in env.State\n",
    "        old = copy(pi_array[s+1])\n",
    "        q_greedify_policy(env, V, pi_array, s, gamma)\n",
    "        if !(isequal(pi_array[s+1], old))\n",
    "            policy_stable = false\n",
    "        end\n",
    "    end\n",
    "    return pi_array, policy_stable\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_iteration (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function policy_iteration(env::ParkingWorld, gamma, theta)\n",
    "    V = zeros(length(env.State))\n",
    "    pi_array = ones(length(env.State), length(env.Action)) / length(env.Action)\n",
    "    policy_stable = false\n",
    "    while !policy_stable\n",
    "        V = evaluate_policy(env, V, pi_array, gamma, theta)\n",
    "        pi_array, policy_stable = improve_policy(env, V, pi_array, gamma)\n",
    "    end\n",
    "    return V, pi_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_greedify_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function q_greedify_policy(env::ParkingWorld, V, pi_array, s, gamma)\n",
    "    \"\"\"Mutate pi_array to be greedy with respect to the q-values induced by V.\"\"\"\n",
    "    # update pi_array\n",
    "    pi_value = zeros(length(env.Action)) # for calculate the argmax\n",
    "    for action in env.Action\n",
    "        trans = transitions(env, s, action) # trainsition value\n",
    "        for state in env.State\n",
    "            pi_value[action+1] = pi_value[action+1] + trans[state + 1, 2] *\n",
    "                                 (trans[state+1, 1] + gamma * V[state+1])\n",
    "        end\n",
    "    end\n",
    "    action_max = argmax(pi_value)\n",
    "    # pi_array: m*n matrix, m represent different state, n represent different\n",
    "    # action respect to different state\n",
    "    pi_array[s+1,:] = zeros(size(pi_array, 2))\n",
    "    # represent 100% probability to take one action locate in action_max\n",
    "    pi_array[s+1, action_max] = 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ParkingWorld(num_spaces=10, num_prices=4)\n",
    "gamma = 0.9\n",
    "theta = 0.1\n",
    "V, pi_array = policy_iteration(env, gamma, theta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Float64,1}:\n",
       " 81.60940116840347\n",
       " 83.28357754451835\n",
       " 85.03018628050616\n",
       " 86.79007707441114\n",
       " 88.51662022628072\n",
       " 90.16819234877124\n",
       " 91.70422112901124\n",
       " 93.08268939887775\n",
       " 94.25817122718998\n",
       " 95.25809637574687\n",
       " 89.45397248620606"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11×4 Array{Float64,2}:\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0\n",
       " 0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tool(V, pi_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the value function (rounded to one decimal place) and policy against the answer below:<br>\n",
    "State $\\quad\\quad$    Value $\\quad\\quad$ Action<br>\n",
    "0 $\\quad\\quad\\quad\\;$        81.6 $\\quad\\quad\\;$ 0<br>\n",
    "1 $\\quad\\quad\\quad\\;$        83.3 $\\quad\\quad\\;$ 0<br>\n",
    "2 $\\quad\\quad\\quad\\;$        85.0 $\\quad\\quad\\;$ 0<br>\n",
    "3 $\\quad\\quad\\quad\\;$        86.8 $\\quad\\quad\\;$ 0<br>\n",
    "4 $\\quad\\quad\\quad\\;$        88.5 $\\quad\\quad\\;$ 0<br>\n",
    "5 $\\quad\\quad\\quad\\;$        90.2 $\\quad\\quad\\;$ 0<br>\n",
    "6 $\\quad\\quad\\quad\\;$        91.7 $\\quad\\quad\\;$ 0<br>\n",
    "7 $\\quad\\quad\\quad\\;$        93.1 $\\quad\\quad\\;$ 0<br>\n",
    "8 $\\quad\\quad\\quad\\;$        94.3 $\\quad\\quad\\;$ 0<br>\n",
    "9 $\\quad\\quad\\quad\\;$        95.3 $\\quad\\quad\\;$ 4<br>\n",
    "10 $\\quad\\quad\\;\\;\\,\\,$      89.5 $\\quad\\quad\\;$ 4<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Value Iteration\n",
    "The city has also heard about value iteration and would like you to implement it. Value iteration works by iteratively applying the Bellman optimality equation for $v_{\\ast}$ to a working value function, as an update rule, as shown below.\n",
    "\n",
    "$$\\large v(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma v(s')]$$\n",
    "We have written an outline of the value iteration algorithm described in chapter 4.4 of the textbook. It is left to you to fill in the `bellman_optimality_update` function to complete the value iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function value_iteration(env::ParkingWorld, gamma, theta)\n",
    "    V = zeros(length(env.State))\n",
    "    while true\n",
    "        delta = 0\n",
    "        for s in env.State\n",
    "            v = V[s+1] \n",
    "            bellman_optimality_update(env, V, s, gamma) \n",
    "            delta = max(delta, abs(v-  V[s+1]))　\n",
    "        end\n",
    "        if delta < theta \n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    pi_array = ones(length(env.State), length(env.Action)) / length(env.Action)\n",
    "    for s in env.State\n",
    "        q_greedify_policy(env, V, pi_array, s, gamma)\n",
    "    end\n",
    "    return V, pi_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bellman_optimality_update (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bellman_optimality_update(env::ParkingWorld, V, s, gamma)\n",
    "    \"\"\"Mutate `V` according to the Bellman optimality update equation\"\"\"\n",
    "    # initial a series to get the max of the sum\n",
    "    state_value = zeros(length(env.Action))\n",
    "    for action in env.Action\n",
    "        trans = transitions(env, s, action)\n",
    "        for state in env.State\n",
    "            state_value[action+1] = state_value[action+1] + trans[state+1,2] *\n",
    "                                    (trans[state+1, 1] + gamma * V[state+1])\n",
    "        end\n",
    "    end\n",
    "    V[s+1] = maximum(state_value)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ParkingWorld(num_spaces=10, num_prices=4)\n",
    "gamma = 0.9\n",
    "theta = 0.1\n",
    "V, pi_array = value_iteration(env, gamma, theta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Float64,1}:\n",
       " 81.60486890609062\n",
       " 83.27914232401172\n",
       " 85.02582906625796\n",
       " 86.78578527433001\n",
       " 88.51238499666755\n",
       " 90.16400717647323\n",
       " 91.70008102620021\n",
       " 93.07859041796065\n",
       " 94.25411015493282\n",
       " 95.2541025364668\n",
       " 89.45001307760359"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11×4 Array{Float64,2}:\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0\n",
       " 0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your value function (rounded to one decimal place) and policy against the answer below:<br>\n",
    "State $\\quad\\quad$    Value $\\quad\\quad$ Action<br>\n",
    "0 $\\quad\\quad\\quad\\;$        81.6 $\\quad\\quad\\;$ 0<br>\n",
    "1 $\\quad\\quad\\quad\\;$        83.3 $\\quad\\quad\\;$ 0<br>\n",
    "2 $\\quad\\quad\\quad\\;$        85.0 $\\quad\\quad\\;$ 0<br>\n",
    "3 $\\quad\\quad\\quad\\;$        86.8 $\\quad\\quad\\;$ 0<br>\n",
    "4 $\\quad\\quad\\quad\\;$        88.5 $\\quad\\quad\\;$ 0<br>\n",
    "5 $\\quad\\quad\\quad\\;$        90.2 $\\quad\\quad\\;$ 0<br>\n",
    "6 $\\quad\\quad\\quad\\;$        91.7 $\\quad\\quad\\;$ 0<br>\n",
    "7 $\\quad\\quad\\quad\\;$        93.1 $\\quad\\quad\\;$ 0<br>\n",
    "8 $\\quad\\quad\\quad\\;$        94.3 $\\quad\\quad\\;$ 0<br>\n",
    "9 $\\quad\\quad\\quad\\;$        95.3 $\\quad\\quad\\;$ 4<br>\n",
    "10 $\\quad\\quad\\;\\;\\,\\,$      89.5 $\\quad\\quad\\;$ 4<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tool(V, pi_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the value iteration algorithm above, a policy is not explicitly maintained until the value function has converged. Below, we have written an identically behaving value iteration algorithm that maintains an updated policy. Writing value iteration in this form makes its relationship to policy iteration more evident. Policy iteration alternates between doing complete greedifications and complete evaluations. On the other hand, value iteration alternates between doing local greedifications and local evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration2 (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function value_iteration2(env::ParkingWorld, gamma, theta)\n",
    "    V = zeros(length(env.State))\n",
    "    pi_array = ones(length(env.State), length(env.Action)) ./ length(env.Action)\n",
    "    while true\n",
    "        delta = 0\n",
    "        for s in env.State\n",
    "            v = V[s+1]\n",
    "            q_greedify_policy(env, V, pi_array, s, gamma)\n",
    "            bellman_update(env, V, pi_array, s, gamma)\n",
    "            delta = max(delta, abs(v - V[s+1]))\n",
    "        end\n",
    "        if delta < theta\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return V, pi_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ParkingWorld(num_spaces=10, num_prices=4)\n",
    "gamma = 0.9\n",
    "theta = 0.1\n",
    "V, pi_array = value_iteration2(env, gamma, theta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Float64,1}:\n",
       " 81.60486890609062\n",
       " 83.27914232401172\n",
       " 85.02582906625796\n",
       " 86.78578527433001\n",
       " 88.51238499666755\n",
       " 90.16400717647323\n",
       " 91.70008102620021\n",
       " 93.07859041796065\n",
       " 94.25411015493282\n",
       " 95.2541025364668\n",
       " 89.45001307760359"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11×4 Array{Float64,2}:\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  1.0\n",
       " 0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tool(V, pi_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
