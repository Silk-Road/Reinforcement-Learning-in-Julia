# encoding = utf-8
# Author: Silk-Road
# Date: 2021-01-01
# Email: swami.liu@outlook.com
# Last modified by: Silk-Road
# Last modified time: 2021-01-03

using StatsBase
using PyCall
using Random
plt = pyimport("matplotlib.pyplot")
np = pyimport("numpy")


include("rl_glue.jl")
include("main_agent.jl")
include("ten_arm_env.jl")
include("utils.jl")

test_array  = [0,0,0,0,0,0,0,0,1,0]

@assert argmax(test_array) == 9 "There is something wrong in `argmax` function"

"""agent does *no* learning, selects action 0 always"""
mutable struct GreedyAgent<:BaseAgent
    last_action
    num_actions
    q_values
    step_size
    epsilon
    initial_value
    arm_count
    current_action
    function GreedyAgent(;last_action=Nothing, num_actions = Nothing, q_values = Nothing,
                   step_size = Nothing, epsilon = Nothing, current_action = Nothing,
                   initial_value=0.0, arm_count=zeros(10))
        agent = new()
        agent.last_action=last_action
        agent.num_actions = num_actions
        agent.q_values = q_values
        agent.step_size = step_size
        agent.epsilon = epsilon
        agent.current_action = current_action
        agent.initial_value = initial_value
        agent.arm_count = arm_count
        agent
    end
end

"""Setup for the agent called when the experiment first starts."""
function agent_init(agent::GreedyAgent; agent_info=Dict())
    agent.num_actions = get(agent_info, "num_actions", 2)
    agent.initial_value = get(agent_info, "initial_value", 0.0)
    agent.q_values = ones(Int(get(agent_info, "num_actions", 2)))*agent.initial_value
    agent.step_size = get(agent_info, "step_size", 0.1)
    agent.epsilon = get(agent_info, "epsilon", 0.0)
    agent.last_action = 0
    return agent
end


"""The first method called when the experiment starts, called after
the environment starts.
Args:
    observation (Numpy array): the state observation from the
        environment's evn_start function.
Returns:
    The first action the agent takes.
"""
function agent_start(agent::GreedyAgent, observation)
    # StatsBase:sample == np.random.choice
    agent.last_action = sample(1:agent.num_actions) # set first action to 0
    return agent.last_action
end

function agent_start(agent::GreedyAgent)
    # StatsBase:sample == np.random.choice
    agent.last_action = sample(1:agent.num_actions) # set first action to 0
    return agent.last_action
end

"""A step taken by the agent.
Args:
    reward (float): the reward received for taking the last action taken
    observation (Numpy array): the state observation from the
        environment's step based, where the agent ended up after the
        last step
Returns:
    The action the agent is taking.
"""
function agent_step(agent::GreedyAgent, reward, observation)
    # local_action = 0 # choose the action here
    #agent.last_action = sample(0:(agent.num_actions-1))
    current_action = agent.last_action
    agent.arm_count[current_action] = agent.arm_count[current_action] + 1
    agent.q_values[current_action] = agent.q_values[current_action] +
                                        1/agent.arm_count[current_action] *
                                        (reward - agent.q_values[current_action])
    current_action = argmax(agent.q_values)
    agent.last_action = current_action
    return current_action
end

"""Run when the agent terminates.
Args:
    reward (float): the reward the agent received for entering the
        terminal state.
"""
function agent_end(agent::GreedyAgent, reward)
end

"""Cleanup done after the agent ends."""
function agent_cleanup(agent::GreedyAgent)
end

"""A function used to pass information from the agent to the experiment.
Args:
    message: The message passed to the agent.
Returns:
    The response (or answer) to the message.
"""
function agent_message(agent::GreedyAgent, message)
end


#=
Section 1: Greedy Agent
=#
greedy_agent = GreedyAgent()
greedy_agent.q_values = [0,0,1.0,0,0]
greedy_agent.arm_count = [0,1,0,0,0]
greedy_agent.last_action = 2 #测试，因为Julia从1开始索引，所以这里last_action为2不为1


# Test for Greedy Agent Code
# agent_start(greedy_agent)
agent_step(greedy_agent, 1,0)
println(greedy_agent.q_values)


# Plot Greedy Result
num_runs = 200                       # The number of times we run the experiment
num_steps = 1000                     # The number of steps each experiment is run
env = Environment                    # The environment to use
agent = GreedyAgent                  # Choose what agent we want to use
agent_info = Dict("num_actions"=>10) # Pass the agent the information it needs
                                     # here it just needs the number of actions (number of arms)
env_info = Dict()
all_averages = []

for i in 1:num_runs
    rl_glue = RLGlue(Environment, GreedyAgent) # Create a new RLGlue  experiment with the env and agent we choose above
    #rl_init(rl_glue, agent_info, env_info)     # Pass RLGlue what it needs to initialize the agent and enviromnet
    rl_start(rl_glue)                          # Start the experiment

    scores = [0.0]
    averages = []
    for i in 1:num_steps
        reward , _, action, _ =rl_step(rl_glue)  # The enviroment and agent take a step and return the reward, and action taken

        append!(scores, scores[end]+reward)
        append!(averages, scores[end]/(i+1))
    end
    append!(all_averages, [averages])
end

#plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')
plt.plot([1.0 for _ in 1:num_steps], linestyle="--")
plt.plot(mean(all_averages))
plt.legend(["Best Possible", "Greedy"])
plt.title("Average Reward of Greedy Agent")
plt.xlabel("Steps")
plt.ylabel("Average reward")
plt.show()
greedy_scores = mean(all_averages)



#=
Section 2: Epsilon-Greedy Agent
=#

mutable struct EpsilonGreedyAgent<:BaseAgent
    last_action
    num_actions
    q_values
    step_size
    epsilon
    initial_value
    arm_count
    current_action
    function EpsilonGreedyAgent(;last_action=Nothing,num_actions = Nothing, q_values = Nothing,
                   step_size = Nothing, epsilon = Nothing, current_action = Nothing,
                   initial_value=0.0, arm_count=zeros(10))
        agent = new()
        agent.last_action=last_action
        agent.num_actions = num_actions
        agent.q_values = q_values
        agent.step_size = step_size
        agent.epsilon = epsilon
        agent.current_action = current_action
        agent.initial_value = initial_value
        agent.arm_count = arm_count
        agent
    end
end

function agent_init(agent::EpsilonGreedyAgent; agent_info=Dict())
    agent.num_actions = get(agent_info, "num_actions", 2)
    agent.initial_value = get(agent_info, "initial_value", 0.0)
    agent.q_values = ones(Int(get(agent_info, "num_actions", 2)))*agent.initial_value
    agent.step_size = get(agent_info, "step_size", 0.1)
    agent.epsilon = get(agent_info, "epsilon", 0.0)
    agent.last_action = 0
    return agent
end



function agent_start(agent::EpsilonGreedyAgent, observation)
    # StatsBase:sample == np.random.choice
    agent.last_action = sample(1:agent.num_actions) # set first action to 0
    return agent.last_action
end

function agent_start(agent::EpsilonGreedyAgent)
    # StatsBase:sample == np.random.choice
    agent.last_action = sample(1:agent.num_actions) # set first action to 0
    return agent.last_action
end

function agent_step(agent::EpsilonGreedyAgent, reward, observation)
#function agent_step(agent::EpsilonGreedyAgent, reward)
    # local_action = 0 # choose the action here
    current_action = agent.last_action
    agent.arm_count[current_action] = agent.arm_count[current_action] + 1
    agent.q_values[current_action] = agent.q_values[current_action] +
                                        1/agent.arm_count[current_action] *
                                        (reward - agent.q_values[current_action])
    rand_num = rand()
    if rand_num < agent.epsilon
        current_action = sample(1:length(agent.q_values))
    else
        current_action = argmax(agent.q_values)
    end
    agent.last_action = current_action
    return current_action
end



function agent_end(agent::EpsilonGreedyAgent, reward)
end

function agent_cleanup(agent::EpsilonGreedyAgent)
end


function agent_message(agent::EpsilonGreedyAgent, message)
end


# 测试
#e_greedy_agent = agent_init(EpsilonGreedyAgent())
e_greedy_agent = EpsilonGreedyAgent()
e_greedy_agent.q_values = [0,0,1.0,0,0]
e_greedy_agent.arm_count = [0,1,0,0,0]
e_greedy_agent.num_actions = 5
e_greedy_agent.last_action = 2#Julia 从1索引
e_greedy_agent.epsilon = 0.5
action = agent_step(e_greedy_agent,1,0)
println(e_greedy_agent.q_values)


# Plot Epsilon greedy results and greedy results

# Plot Greedy Result
num_runs = 200                       # The number of times we run the experiment
num_steps = 1000                     # The number of steps each experiment is run
epsilon = 0.1
env = Environment                    # The environment to use
agent = EpsilonGreedyAgent                  # Choose what agent we want to use
agent_info = Dict("num_actions"=>10, "epsilon"=>epsilon) # Pass the agent the information it needs
                                     # here it just needs the number of actions (number of arms)
env_info = Dict()
all_averages = []

for i in 1:num_runs
    rl_glue = RLGlue(env, agent) # Create a new RLGlue  experiment with the env and agent we choose above
    #rl_init(rl_glue, agent_info, env_info)     # rl_init has merge into the definition of RLGlue
    rl_start(rl_glue)                          # Start the experiment

    scores = [0.0]
    averages = []
    for i in 1:num_steps
        reward , _, action, _ =rl_step(rl_glue)  # The enviroment and agent take a step and return the reward, and action taken

        append!(scores, scores[end]+reward)
        append!(averages, scores[end]/(i+1))
    end
    append!(all_averages, [averages])
end

plt.plot([1.0 for _ in 1:num_steps], linestyle="--")
plt.plot(greedy_scores)
plt.title("Average Reward of Greedy Agent vs. Epsilon-Greedy Agent")
plt.plot(mean(all_averages))
plt.legend(("Best Possible", "Greedy", "Epsilon Greedy: Epsilon = 0.1"))
plt.xlabel("Steps")
plt.ylabel("Average reward")
plt.show()


#=
1.2 Averaging Multiple Runs
=#
# Plot runs of e-greedy agent
env = Environment
agent = EpsilonGreedyAgent
agent_info = Dict("num_actions"=> 10, "epsilon"=> 0.1)
env_info = Dict()
all_averages = []
#plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')
num_steps = 1000

for run in (0, 1)
    Random.seed!(run) # Here we set the seed so that we can compare two different runs
    averages = []
    rl_glue = RLGlue(env, agent)
    rl_start(rl_glue)

    scores = [0.0]
    for i in 1:num_steps
        reward, state, action, is_terminal = rl_step(rl_glue)
        append!(scores, scores[end] + reward)
        append!(averages, scores[end] / (i + 1))
    end
#     all_averages.append(averages)
    plt.plot(averages)
end
# plt.plot(greedy_scores)
plt.title("Comparing two runs of the same agent")
plt.xlabel("Steps")
plt.ylabel("Average reward")
# plt.plot(np.mean(all_averages, axis=0))
# plt.legend(("Greedy", "Epsilon: 0.1"))
plt.show()



#=
Section 3: Comparing values of epsilon
=#
epsilons = [0.0, 0.01, 0.1, 0.4]
n_q_values = []
n_averages = []
n_best_actions = []
num_steps = 1000
num_runs = 200

for epsilon in epsilons
    all_averages = []
    for run in 1:num_runs
        agent = EpsilonGreedyAgent
        env = Environment
        agent_info = Dict("num_action"=>10, "epsilon"=>epsilon)
        env_info = Dict("random_seed"=>run)

        rl_glue = RLGlue(env, agent)
        rl_start(rl_glue)

        best_arm = argmax(rl_glue.environment.arms)

        scores = [0.0]
        averages = []
        best_action_chosen = []

        for i in 1:num_steps
            reward, state, action, is_terminal = rl_step(rl_glue)
            append!(scores, scores[end]+reward)
            append!(averages, scores[end]/(i+1))
            if action == best_arm
                append!(best_action_chosen, 1)
            else
                append!(best_action_chosen, 0)
            end
            if epsilon==0.1 && run == 0
                append!(n_q_values, copy(rl_glue.agenet.q_values))
            end
        end

        if epsilon == 0.1
            append!(n_averages, averages)
            append!(n_best_actions, best_action_chosen)
        end
        append!(all_averages, [averages])
    end
    plt.plot(mean(all_averages))
end
plt.legend(["Best Possible"] + epsilons)
plt.xlabel("Steps")
plt.ylabel("Average reward")
plt.show()



#=
Section 4: The Effect of Step Size
=#
mutable struct EpsilonGreedyAgentConstantStepsize<:BaseAgent
    last_action
    num_actions
    q_values
    step_size
    epsilon
    initial_value
    arm_count
    current_action
    function EpsilonGreedyAgentConstantStepsize(;last_action=Nothing,num_actions = Nothing, q_values = Nothing,
                   step_size = Nothing, epsilon = Nothing, current_action = Nothing,
                   initial_value=0.0, arm_count=zeros(10))
        agent = new()
        agent.last_action=last_action
        agent.num_actions = num_actions
        agent.q_values = q_values
        agent.step_size = step_size
        agent.epsilon = epsilon
        agent.current_action = current_action
        agent.initial_value = initial_value
        agent.arm_count = arm_count
        agent
    end
end

function agent_init(agent::EpsilonGreedyAgentConstantStepsize; agent_info=Dict())
    agent.num_actions = get(agent_info, "num_actions", 2)
    agent.initial_value = get(agent_info, "initial_value", 0.0)
    agent.q_values = ones(Int(get(agent_info, "num_actions", 2)))*agent.initial_value
    agent.step_size = get(agent_info, "step_size", 0.1)
    agent.epsilon = get(agent_info, "epsilon", 0.0)
    agent.last_action = 0
    return agent
end

function agent_start(agent::EpsilonGreedyAgentConstantStepsize, observation)
    # StatsBase:sample == np.random.choice
    agent.last_action = sample(1:agent.num_actions) # set first action to 0
    return agent.last_action
end

function agent_start(agent::EpsilonGreedyAgentConstantStepsize)
    # StatsBase:sample == np.random.choice
    agent.last_action = sample(1:agent.num_actions) # set first action to 0
    return agent.last_action
end

function agent_step(agent::EpsilonGreedyAgentConstantStepsize, reward, observation)
#function agent_step(agent::EpsilonGreedyAgent, reward)
    # local_action = 0 # choose the action here
    current_action = agent.last_action
    agent.arm_count[current_action] = agent.arm_count[current_action] + 1
    agent.q_values[current_action] = agent.q_values[current_action] +
                                        1/agent.step_size *
                                        (reward - agent.q_values[current_action])
    rand_num = rand()
    if rand_num < agent.epsilon
        current_action = sample(1:length(agent.q_values))
    else
        current_action = argmax(agent.q_values)
    end
    agent.last_action = current_action
    return current_action
end


function agent_end(agent::EpsilonGreedyAgentConstantStepsize, reward)
end

function agent_cleanup(agent::EpsilonGreedyAgentConstantStepsize)
end


function agent_message(agent::EpsilonGreedyAgentConstantStepsize, message)
end

for step_size in [0.01, 0.1, 0.5, 1.0]
    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()
    e_greedy_agent.q_values = [0,0,1.0,0,0]
    e_greedy_agent.num_actions = 5
    e_greedy_agent.last_action = 1
    e_greedy_agent.epsilon = 0.0
    e_greedy_agent.step_size = step_size
    action = agent_step(e_greedy_agent,1,0)
    println(e_greedy_agent.q_values)
end





epsilon = 0.1
num_steps = 1000
num_runs = 200

#fig, ax = plt.subplots(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')
fig, ax = plt.subplots()


step_sizes = [0.01, 0.1, 0.5, 1.0]
q_values = Dict(step_size => [] for step_size in step_sizes)
true_values = Dict(step_size => Nothing for step_size in step_sizes)
best_actions = Dict(step_size => [] for step_size in step_sizes)


for step_size in step_sizes
    all_averages = []
    for run in 1:num_runs
        env = Environment
        agent = EpsilonGreedyAgentConstantStepsize
        agent_info = Dict("num_actions"=>10, "epsilon"=>epsilon,
                          "step_size"=>step_size, "initial_value"=>0.0)
        env_info = Dict("random_seed"=>run)

        rl_glue = RLGlue(env, agent)
        rl_start(rl_glue)

        best_arm = argmax(rl_glue.environment.arms)
        scores = [0.0]
        averages = []

        if run==0
            true_values[step_size] = copy(rl_glue.environment.arms)
        end

        best_action_chosen = []
        for i in 1:num_steps
            reward, state, action, is_terminal = rl_step(rl_glue)
            append!(scores, scores[end] + reward)
            append!(averages, scores[end]/(i+1))
            if action == best_arm
                append!(best_action_chosen, 1)
            else
                append!(best_action_chosen, 0)
            end
            if run == 0
                append!(q_values[step_size], copy(rl_glue.agent.q_values))
            end
        end
        append!(best_actions[step_size], [best_action_chosen])

    end
    #println(mean(best_actions[step_size]), "\n")
    ax.plot(mean(best_actions[step_size]))
end
ax.plot(mean(n_best_actions))
fig.legend(step_sizes * ["1/N(A)"])
plt.title("% Best Action Taken")
plt.xlabel("Steps")
plt.ylabel("% Best Action Taken")
vals = ax.get_yticks()
ax.set_yticklabels(['{:,.2%}'.format(x) for x in vals])
plt.show()
